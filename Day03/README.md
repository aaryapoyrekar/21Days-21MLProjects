# ğŸ  Day 03 â€“ House Price Prediction (Regression)

## ğŸ“Œ Project Overview
This project focuses on building an **end-to-end regression pipeline** to predict house sale prices using the **Ames Housing Dataset**.  
It covers the complete machine learning workflow â€” from exploratory data analysis and feature engineering to model training, evaluation, and Kaggle submission.

---

## ğŸ¯ Objective
To accurately predict the **SalePrice** of houses based on multiple numerical and categorical features, and improve model performance through **feature engineering and hyperparameter tuning**.

---

## ğŸ“‚ Dataset
- Source: **Kaggle â€“ House Prices: Advanced Regression Techniques**
- Files used:
  - `train.csv`
  - `test.csv`

---

## ğŸ” Exploratory Data Analysis (EDA)
- Analyzed the distribution of the target variable `SalePrice`
- Identified **positive skewness** and applied **log transformation**
- Used correlation heatmaps to identify features strongly related to price  
  (`OverallQual`, `GrLivArea`, `GarageCars`, `GarageArea`)

---

## ğŸ›  Data Preprocessing
- Handled missing values using:
  - Zero imputation for numerical absence-based features
  - Median imputation by neighborhood for `LotFrontage`
  - Mode / `"None"` imputation for categorical features
- Combined train and test datasets to ensure **consistent preprocessing**
- Applied **One-Hot Encoding** for categorical variables

---

## âš™ï¸ Feature Engineering
New features created to improve model performance:
- `TotalSF` â€“ Total square footage
- `TotalBath` â€“ Total number of bathrooms
- `Age` â€“ Age of the house at the time of sale
- `RemodAge` â€“ Years since last remodel
- `TotalPorchSF` â€“ Total porch area
- `TotalRooms` â€“ Total rooms including bedrooms
- `HasGarage` â€“ Binary indicator for garage presence
- `HasBasement` â€“ Binary indicator for basement presence

---

## ğŸ¤– Models Used
### 1ï¸âƒ£ Linear Regression (Baseline)
- Used as a simple baseline model
- Limited performance due to non-linear relationships

### 2ï¸âƒ£ XGBoost Regressor (Final Model)
- Tuned hyperparameters for better generalization
- Captures complex non-linear interactions
- Significantly outperformed Linear Regression

---

## ğŸ“Š Model Evaluation Metrics
- **RMSE (Root Mean Squared Error)**
- **MAE (Mean Absolute Error)**
- **RÂ² Score**

âœ” XGBoost achieved lower error and higher RÂ² compared to the baseline model.

---

## ğŸ“¦ Submission
- Generated `submission.csv`
- Reversed log transformation before final prediction
- Successfully submitted predictions to Kaggle

---

## ğŸ§  Key Learnings
- Feature engineering has a major impact on model accuracy
- Log transformation helps stabilize skewed target variables
- Tree-based models perform better on complex tabular datasets
- Consistent preprocessing prevents data leakage

---

## ğŸš€ Future Improvements
- Hyperparameter tuning using GridSearchCV
- Advanced feature interactions
- Ensemble modeling for further performance gains

---

## âœ… Status
âœ” Project Completed  
âœ” Accuracy Improved  
âœ” Kaggle Submission Successful  

---

ğŸ“Œ **Part of the series:** *21 Days â€“ 21 Machine Learning Projects*
